{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fec21175-266c-45ca-a1e1-ca0c43fa01e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_path):\n",
    "    try:\n",
    "        with open(file_path) as file:\n",
    "            return file.read()\n",
    "    except FileNotFoundError as err:\n",
    "        print('Check file path!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67289702-82dd-4731-b2ed-482cb61844d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/home/viper/Downloads/UPDATED_NLP_COURSE/TextFiles/moby_dick_four_chapters.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fce567c-a9c1-4992-b6d1-4810fe7e60aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-19 01:24:47.310769: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-19 01:24:47.685771: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-01-19 01:24:47.685803: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-01-19 01:24:49.489993: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-19 01:24:49.490144: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-19 01:24:49.490161: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-01-19 01:24:52.101998: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-01-19 01:24:52.102236: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-01-19 01:24:52.102268: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (viper): /proc/driver/nvidia/version does not exist\n"
     ]
    }
   ],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9266a40e-e5d4-41b9-aece-18551770337d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'tagger', 'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f9f41c4-8b26-46ec-8325-09efaa7739da",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.max_length = 1200000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15e371b2-60cb-4eb6-abdf-cfc36206d601",
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_punc(doc_text):\n",
    "    return [token.text.lower() for token in nlp(doc_text) if token.text not in '\\n\\n \\n\\n\\n!\"-#$%&()--.*+,-/:;<=>?@[\\\\]^_`{|}~\\t\\n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e899c234-574c-4fe7-8353-36fbf557aab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = read_file(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4bc6e7b-f15b-4cdf-a196-8a95f9b5d957",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/viper/Ranjan/Coding/NaturalLanguageProcessing/natural_language_processing/venv/lib/python3.10/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    }
   ],
   "source": [
    "tokens = separate_punc(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "486c1460-5607-4510-94ed-4f45158af50a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nGiven first 25 words --> network to predict 26th word.\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Given first 25 words --> network to predict 26th word.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74594bef-c54f-4394-bbe7-f0384075ef23",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_len = 25 + 1\n",
    "text_sequences = []\n",
    "\n",
    "for i in range(train_len, len(tokens)):\n",
    "    text_sequence = tokens[i - train_len : i]\n",
    "    text_sequences.append(text_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de471443-5d09-4131-a45f-2e7e422867b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a424460b-4bfd-4516-ac8a-a912e16ddd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee7a4554-f39e-4148-8828-f80b2ffc6651",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(text_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "298d8daf-4866-47a7-909c-a2c061587b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(text_sequences)\n",
    "# text converted to sequence of numbers or indices of text\n",
    "# each of the number in sequence is id of a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "214ef8c8-e0b2-480a-9304-2fc30beae462",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index_dict = tokenizer.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c73f3283-40e0-4d6a-bdb5-0a5d6304adab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "956 --> call\n",
      "14 --> me\n",
      "263 --> ishmael\n",
      "51 --> some\n",
      "261 --> years\n",
      "408 --> ago\n",
      "87 --> never\n",
      "219 --> mind\n",
      "129 --> how\n",
      "111 --> long\n",
      "954 --> precisely\n",
      "260 --> having\n",
      "50 --> little\n",
      "43 --> or\n",
      "38 --> no\n",
      "314 --> money\n",
      "7 --> in\n",
      "23 --> my\n",
      "546 --> purse\n",
      "3 --> and\n",
      "150 --> nothing\n",
      "259 --> particular\n",
      "6 --> to\n",
      "2713 --> interest\n",
      "14 --> me\n",
      "24 --> on\n"
     ]
    }
   ],
   "source": [
    "for id in sequences[0]:\n",
    "    print(f\"{id} --> {tokenizer.index_word[id]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3edef45f-8b88-4c0a-9315-a6ae8e372e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = tokenizer.word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5a057200-ae5a-4c56-bd69-7c460d6a5408",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = len(tokenizer.word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "acacbcac-11fb-4b9f-b0c8-3d5813d266f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2718"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "58946b9f-182a-4915-853a-8b6fae970568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting id sequences to matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3ea45aef-c610-4e5b-91ec-6027623a9db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "25a98557-fb88-4ddc-80bd-1ff358e53c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3d4e4ada-eb87-4685-8497-599551ad6a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8b808357-691c-4d16-b9e6-21a8a4ccc48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sequences[:,:-1]\n",
    "y = sequences[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1ade83c6-f1a4-4303-beb2-8163ac823a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(y, num_classes=vocabulary_size + 1) \n",
    "#  num_classes=vocabulary_size + 1 --> because Keras needs one more class to hold 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "92ea707d-b5f6-4a24-83c2-7fd760732079",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11312, 2719)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "75c732f5-5efb-4e20-bb25-0b105ff602f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c9ba67b5-5e6b-418e-a46f-282067c1b6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "8a3576b1-2b6c-4522-9222-9af812947428",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(vocabulary_size: int, seq_len: int):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=vocabulary_size, output_dim=seq_len, input_length=seq_len))\n",
    "    model.add(LSTM(seq_len*2, return_sequences=True))\n",
    "    model.add(LSTM(seq_len*2))\n",
    "    \n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    \n",
    "    model.add(Dense(vocabulary_size, activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "2cc82c36-48ff-41c7-88e9-fb2ddb5cb4ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 25, 25)            67975     \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 25, 50)            15200     \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 50)                20200     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 50)                2550      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 2719)              138669    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 244,594\n",
      "Trainable params: 244,594\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model(vocabulary_size+1, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c6e30fb4-80f5-4be4-88e9-fd2b28a253ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "2ce455e9-9d94-43c9-b424-16c4e4acf4b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4f0858b070>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, batch_size=128, epochs=20, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "1492a4ae-9f38-41a0-af61-195451c5285e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('mobi_dick_lstm_20epoch.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b6f240b5-c1b6-42f3-84e6-1aca69c435c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(tokenizer, open('mobi_dick_tokenizer', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "74d5cbf7-442d-4c0f-bacf-ab3ca2db5d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "bf433b71-7455-44bb-a564-d40eb4b7d599",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, seq_len, seed_text, num_gen_words):\n",
    "    \n",
    "    output_text = []\n",
    "    input_text = seed_text\n",
    "    for i in range(num_gen_words):\n",
    "        encoded_text = tokenizer.texts_to_sequences([input_text])[0]\n",
    "        \n",
    "        pad_encoded = pad_sequences([encoded_text], maxlen=seq_len, truncating='pre')\n",
    "        \n",
    "        pred_word_index = model.predict(pad_encoded, verbose=0)\n",
    "        \n",
    "        pred_word_index = np.argmax(pred_word_index,axis=1)[0]\n",
    "\n",
    "        pre_word = tokenizer.index_word[pred_word_index]\n",
    "\n",
    "        input_text = ' ' + pre_word\n",
    "        output_text.append(pre_word)\n",
    "\n",
    "    return ' '.join(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "5a697d33-1151-44f8-8c36-26508bb1ff18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(101)\n",
    "random_pick = random.randint(0, len(text_sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "9e9325ea-fe7b-4e7f-a43e-9b8ba07cbf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed_text = text_sequences[random_pick]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "828b4e1d-18e5-4240-a9b2-4ef270b41c77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"thought i to myself the man 's a human being just as i am he has just as much reason to fear me as i have\""
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_text = ' '.join(random_seed_text)\n",
    "seed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "843a12a0-f9f5-41ae-847e-8e7f0bb24b9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a little harpooneer the little harpooneer the little harpooneer the little harpooneer the little harpooneer the little harpooneer the little harpooneer the little harpooneer the'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, seq_len, seed_text, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190e33c6-d983-457d-9ad5-f0801b01ca9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "daaa503d-77eb-4de0-8c49-79af33b110bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "97373c46-c9dc-42c8-9e4a-3bfffae8ff58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "45ee51f8-41d4-48fc-a3c1-dedd3d05200c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('/home/viper/Downloads/UPDATED_NLP_COURSE/06-Deep-Learning/epochBIG.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "59b8dadd-01cf-4632-a4a9-668d6d467abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = load(open('/home/viper/Downloads/UPDATED_NLP_COURSE/06-Deep-Learning/epochBIG', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "628a5753-e8c4-4542-b57e-48fd30e6e657",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'to him me whatever whatever whatever whatever whatever whatever whatever whatever whatever whatever whatever whatever whatever whatever whatever whatever whatever whatever whatever whatever whatever whatever'"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, seq_len, seed_text, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61685dec-c233-47c4-86aa-be6a1efacec9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
