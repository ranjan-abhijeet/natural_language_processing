{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8d63754-05fb-4ff5-bf36-6cbd1d0918c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTopic modeling allows us to analyze large volumes\\nof text by clustering documents into topics.\\n\\nIf we have unlabeled data, the we can \\'discover\\' labels.\\nIn the case of text data, this means \\'discovering\\' clusters\\nof documents, grouped together by topic.\\n\\nAssumptions of Latent Dirichlet Allocation:\\n    1. Documents are probability distributions over latent(undiscovered) topics.\\n       --> a document can belong to multiple topics with different probabilities.\\n       \\n    2. Topics themselves are probability distributions over words.\\n\\nHow Latent Dirichlet Allocation assumes we produce documents:\\n    1. We first decide the number of words N the document will have.\\n    2. Choose a mixture of topics for the document(according to a Dirichlet\\n       distribution over a fixed set of K topics). \\n       e.g. 60% business, 20% politics, 10% food.\\n    3. Then we generate each word in the document by picking a topic\\n       according to the multinomial distribution that we sampled previously\\n       (60% business, 20% politics, 10% food)\\n    4. Using the topic we generate the word itselt(as per the topic\\'s distribution)\\n       e.g. if we selected the food topic, we might generate the word \"biryani\"\\n       with 60% probability, \"home\" with 30% probability, and so on.\\n    5. ASsuming this type of generative model for a collection of documents, LDA\\n       then tries to backtrack from the documents to find a set of topics that are\\n       likely to have generated the collection.\\n    \\nWe have to tell LDA how many topics(K) needs to be discovered.\\n\\nFor every word in every document and for each topic t we calculate:\\n    p(word w | topic t) --> probability of word w coming from topic t.\\n    p(topic t | document d) --> probability of topic t coming from document d.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Topic modeling allows us to analyze large volumes\n",
    "of text by clustering documents into topics.\n",
    "\n",
    "If we have unlabeled data, the we can 'discover' labels.\n",
    "In the case of text data, this means 'discovering' clusters\n",
    "of documents, grouped together by topic.\n",
    "\n",
    "Assumptions of Latent Dirichlet Allocation:\n",
    "    1. Documents are probability distributions over latent(undiscovered) topics.\n",
    "       --> a document can belong to multiple topics with different probabilities.\n",
    "       \n",
    "    2. Topics themselves are probability distributions over words.\n",
    "\n",
    "How Latent Dirichlet Allocation assumes we produce documents:\n",
    "    1. We first decide the number of words N the document will have.\n",
    "    2. Choose a mixture of topics for the document(according to a Dirichlet\n",
    "       distribution over a fixed set of K topics). \n",
    "       e.g. 60% business, 20% politics, 10% food.\n",
    "    3. Then we generate each word in the document by picking a topic\n",
    "       according to the multinomial distribution that we sampled previously\n",
    "       (60% business, 20% politics, 10% food)\n",
    "    4. Using the topic we generate the word itselt(as per the topic's distribution)\n",
    "       e.g. if we selected the food topic, we might generate the word \"biryani\"\n",
    "       with 60% probability, \"home\" with 30% probability, and so on.\n",
    "    5. ASsuming this type of generative model for a collection of documents, LDA\n",
    "       then tries to backtrack from the documents to find a set of topics that are\n",
    "       likely to have generated the collection.\n",
    "    \n",
    "We have to tell LDA how many topics(K) needs to be discovered.\n",
    "\n",
    "For every word in every document and for each topic t we calculate:\n",
    "    p(word w | topic t) --> probability of word w coming from topic t.\n",
    "    p(topic t | document d) --> probability of topic t coming from document d.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91848b2d-2852-4ba5-9eeb-824d20f4a781",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"/home/viper/Downloads/UPDATED_NLP_COURSE/TextFiles/npr.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b6c633f-b616-4861-b9a1-f1c64772bd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da76acd0-5aa0-45e1-9616-d53ef4feae7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "npr = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "484bea17-bc98-4be3-9f95-26dbb554b9ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In the Washington of 2016, even when the polic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Donald Trump has used Twitter  —   his prefe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Donald Trump is unabashedly praising Russian...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Updated at 2:50 p. m. ET, Russian President Vl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From photography, illustration and video, to d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Article\n",
       "0  In the Washington of 2016, even when the polic...\n",
       "1    Donald Trump has used Twitter  —   his prefe...\n",
       "2    Donald Trump is unabashedly praising Russian...\n",
       "3  Updated at 2:50 p. m. ET, Russian President Vl...\n",
       "4  From photography, illustration and video, to d..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "343ae8a9-494f-4654-87c8-5f3f09cb8759",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4611a88f-e159-4add-afab-f305171fa73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(max_df=0.9, min_df=2, stop_words='english') \n",
    "# max_df = 0.9 => discard the words which appear in more than 90% of the documents.\n",
    "# min_df = 2 => only keep words which appear in atleast 2 documents.\n",
    "# stop_words = 'english' => CountVectorizer will not use 'english' stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cc9d2ecf-367a-42d1-b964-4346f3669ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_term_metric = cv.fit_transform(npr['Article'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f5458f3a-92ba-402e-a8ab-c3a276dd1a72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11992, 54777)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_term_metric.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bee400a6-384c-438f-acd4-6d8db5e57fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "17b38beb-4a22-4b64-aa42-c6b30b588417",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_components=7, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e9b47477-f8e5-4c90-a0b9-ea353923743d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LatentDirichletAllocation(n_components=7, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LatentDirichletAllocation</label><div class=\"sk-toggleable__content\"><pre>LatentDirichletAllocation(n_components=7, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LatentDirichletAllocation(n_components=7, random_state=42)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.fit(document_term_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f05e62a-a0f1-4b2b-bc82-bd4f609a0087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the vocabulary of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "103e8cbb-bb59-41ca-a680-9af917854697",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54777"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cv.get_feature_names_out()) # list of all the words in the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8761b87f-82be-408b-b087-cf8840aa054c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "af0a2a05-e085-41eb-a803-39f23839a851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smallness\n",
      "debilitating\n",
      "emerges\n",
      "faction\n",
      "community\n",
      "lakeland\n",
      "nagging\n",
      "antihistamine\n",
      "deadlocks\n",
      "loin\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    index = random.randint(0, len(cv.get_feature_names_out()))\n",
    "    print(cv.get_feature_names_out()[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f81f1e1b-cad9-4937-b8e2-380b6c320398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5322cfc5-81f1-4a9c-9cec-57d6c1a64f59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lda.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "19e9d058-e654-447d-ae27-98298be2514a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(lda.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "69722961-356a-42b9-a917-2cd03ba4e0f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 54777)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.components_.shape # topic X probability of each word in the topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b3706d4a-2771-478b-a47c-e305f0d8e991",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_topic = lda.components_[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7d1857e0-e71f-460c-83f3-0811340b6db7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([34110, 24223, 23865, ..., 36283, 28659, 42993])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_topic.argsort() # returns the indices in order which will sort the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d75fbefe-3a4d-4942-a199-eabd37d5b75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_twenty_words = single_topic.argsort()[-20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "00bf179a-6bba-4686-9d35-c5f81efafe32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "little\n",
      "know\n",
      "don\n",
      "year\n",
      "make\n",
      "way\n",
      "world\n",
      "family\n",
      "home\n",
      "day\n",
      "time\n",
      "water\n",
      "city\n",
      "new\n",
      "years\n",
      "food\n",
      "just\n",
      "people\n",
      "like\n",
      "says\n"
     ]
    }
   ],
   "source": [
    "for word in top_twenty_words:\n",
    "    print(cv.get_feature_names_out()[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "84eb18b6-13ce-4973-9efb-e206e8671aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the highest probability words per topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7eda3d82-f21c-4465-88b4-4dd451fb9baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE TOP 15 WORDS FOR TOPIC #1\n",
      "['companies', 'money', 'year', 'federal', '000', 'new', 'percent', 'government', 'company', 'million', 'care', 'people', 'health', 'said', 'says']\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #2\n",
      "['military', 'house', 'security', 'russia', 'government', 'npr', 'reports', 'says', 'news', 'people', 'told', 'police', 'president', 'trump', 'said']\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #3\n",
      "['way', 'world', 'family', 'home', 'day', 'time', 'water', 'city', 'new', 'years', 'food', 'just', 'people', 'like', 'says']\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #4\n",
      "['time', 'new', 'don', 'years', 'medical', 'disease', 'patients', 'just', 'children', 'study', 'like', 'women', 'health', 'people', 'says']\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #5\n",
      "['voters', 'vote', 'election', 'party', 'new', 'obama', 'court', 'republican', 'campaign', 'people', 'state', 'president', 'clinton', 'said', 'trump']\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #6\n",
      "['years', 'going', 've', 'life', 'don', 'new', 'way', 'music', 'really', 'time', 'know', 'think', 'people', 'just', 'like']\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #7\n",
      "['student', 'years', 'data', 'science', 'university', 'people', 'time', 'schools', 'just', 'education', 'new', 'like', 'students', 'school', 'says']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, topic in enumerate(lda.components_):\n",
    "    print(f\"THE TOP 15 WORDS FOR TOPIC #{i+1}\")\n",
    "    print([cv.get_feature_names_out()[index] for index in topic.argsort()[-15:]])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "80c63bab-42d4-4b5e-8825-6373547c51a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning topic numbers to Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ebf34609-229f-40be-8e8c-9ba7f571ba7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_results = lda.transform(document_term_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "daa8f012-66ad-4de1-ad28-b422c1e01e18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_results[0].round(3).argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c969ae63-02a8-4e74-bb6a-6e8e5df95f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "npr['Topic'] = topic_results.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5f1b04db-c3a1-4a0d-82b1-0c50aca3fe0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article</th>\n",
       "      <th>Topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In the Washington of 2016, even when the polic...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Donald Trump has used Twitter  —   his prefe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Donald Trump is unabashedly praising Russian...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Updated at 2:50 p. m. ET, Russian President Vl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From photography, illustration and video, to d...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Article  Topic\n",
       "0  In the Washington of 2016, even when the polic...      1\n",
       "1    Donald Trump has used Twitter  —   his prefe...      1\n",
       "2    Donald Trump is unabashedly praising Russian...      1\n",
       "3  Updated at 2:50 p. m. ET, Russian President Vl...      1\n",
       "4  From photography, illustration and video, to d...      2"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c4baba74-d9f9-4fd1-9565-f4b4748da913",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    2458\n",
       "1    2004\n",
       "4    1943\n",
       "2    1838\n",
       "3    1485\n",
       "0    1313\n",
       "6     951\n",
       "Name: Topic, dtype: int64"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npr['Topic'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43b9e53-c62e-4267-9971-613e9487c17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "value_counts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
